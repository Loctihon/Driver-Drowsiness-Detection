import pandas as pd
import joblib
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, accuracy_score

# --- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ---
# D√πng ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ tr√°nh l·ªói m√°y kh√°c nhau
current_dir = os.path.dirname(os.path.abspath(__file__))
CSV_FILE = os.path.join(current_dir, "geometry_features.csv")
MODEL_PATH = os.path.join(current_dir, "drowsiness_ensemble.pkl")

print("[1] üì• ƒêang t·∫£i d·ªØ li·ªáu...")
try:
    df = pd.read_csv(CSV_FILE)
    print(f"-> ƒê√£ t·∫£i {len(df)} d√≤ng d·ªØ li·ªáu.")
except FileNotFoundError:
    print(f"‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y file {CSV_FILE}")
    print("-> H√£y ch·∫°y gom_file.py ƒë·ªÉ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc!")
    exit()

# L·∫•y d·ªØ li·ªáu ƒë·∫ßu v√†o (Features) v√† nh√£n (Label)
X = df[["LeftEAR", "RightEAR", "MAR"]]
y = df["Label"]

# Chia t·∫≠p train/test (80% h·ªçc, 20% thi)·∫Ω
# stratify=y: ƒê·∫£m b·∫£o t·ª∑ l·ªá c√°c nh√£n (Ng√°p, Ng·ªß, B√¨nh th∆∞·ªùng) ·ªü t·∫≠p train v√† test gi·ªëng nhau
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("[2] ‚öôÔ∏è ƒêang thi·∫øt l·∫≠p ki·∫øn tr√∫c 'Si√™u Model' (Ensemble)...")

# --- K·ª∏ THU·∫¨T 1: PIPELINE & SCALING (M·ªöI) ---
# SVM r·∫•t nh·∫°y c·∫£m v·ªõi d·ªØ li·ªáu ch∆∞a chu·∫©n h√≥a.
# Ta t·∫°o m·ªôt 'ƒë∆∞·ªùng ·ªëng' (Pipeline): D·ªØ li·ªáu ƒëi qua Scaler (l√†m s·∫°ch) -> r·ªìi m·ªõi v√†o SVM.
svm_pipeline = Pipeline([
    ('scaler', StandardScaler()), # Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ d·∫°ng chu·∫©n (Mean=0, Std=1)
    ('svm', SVC(kernel='rbf', C=10, gamma='scale', probability=True, class_weight='balanced'))
])

# --- K·ª∏ THU·∫¨T 2: RANDOM FOREST (GI·ªÆ NGUY√äN) ---
# Random Forest kh√¥ng c·∫ßn Scale, n√≥ gi·ªèi x·ª≠ l√Ω nhi·ªÖu.
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

# --- K·ª∏ THU·∫¨T 3: GRADIENT BOOSTING (M·ªöI - C·ª∞C M·∫†NH) ---
# Model n√†y h·ªçc theo ki·ªÉu "S·ª≠a sai". N√≥ nh√¨n xem c√°c model tr∆∞·ªõc sai ·ªü ƒë√¢u ƒë·ªÉ t·∫≠p trung h·ªçc ch·ªó ƒë√≥.
gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)

# --- T·ªîNG H·ª¢P: VOTING CLASSIFIER (H·ªòI ƒê·ªíNG GI√ÅM KH·∫¢O) ---
# K·∫øt h·ª£p c·∫£ 3 √¥ng l·ªõn: SVM (To√°n h·ªçc) + Random Forest (Th·ªëng k√™) + Gradient Boosting (H·ªçc s√¢u chu·ªói)
voting_clf = VotingClassifier(
    estimators=[
        ('svm_pipe', svm_pipeline), 
        ('rf', rf_clf),
        ('gb', gb_clf)
    ],
    voting='soft', # 'soft': T√≠nh trung b√¨nh ƒë·ªô tin c·∫≠y (x√°c su·∫•t) thay v√¨ ch·ªâ ƒë·∫øm phi·∫øu b·∫ßu
    weights=[2, 1, 1] # (Tu·ª≥ ch·ªçn) Cho SVM quy·ªÅn l·ª±c g·∫•p ƒë√¥i n·∫øu n√≥ ch√≠nh x√°c nh·∫•t
)

print("[3] üß† ƒêang hu·∫•n luy·ªán (Training)...")
voting_clf.fit(X_train, y_train)

# ƒê√°nh gi√° k·∫øt qu·∫£
print("\n--- üìä K·∫æT QU·∫¢ ƒê√ÅNH GI√Å MODEL ---")
predictions = voting_clf.predict(X_test)
acc = accuracy_score(y_test, predictions)
print(f"ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {acc*100:.2f}%")
print(classification_report(y_test, predictions, target_names=["Normal", "Sleep", "Yawn"]))

# L∆∞u model
joblib.dump(voting_clf, MODEL_PATH)
print(f"‚úÖ ƒê√£ l∆∞u model th√†nh c√¥ng t·∫°i: {MODEL_PATH}")
print("-> Model m·ªõi ƒë√£ t√≠ch h·ª£p b·ªô chu·∫©n h√≥a (Scaler) b√™n trong.")
print("-> B·∫°n kh√¥ng c·∫ßn s·ª≠a code run_realtime.py, c·ª© ch·∫°y l√† n√≥ t·ª± hi·ªÉu!")